# Ejercicio 1

El corpus elegido es un conjunto de entrevistas a empresarios funcionarios y vecinos
de Asunción, La Paz y Córdoba. Las entrevistas fueron desgrabadas por licenciados en
ciencias políticas durante su doctorado.

## Scripts para cambiar convertir formato y codificación.
+ Convertir rtf a txt: `find ./ -iname "*.rtf" -type f -exec sh -c 'unrtf --text "${0}">"${0%.rtf}.txt"'  {} \;`
+ Convertir formato a utf-8: `find ./ -iname "*.txt" -type f -exec sh -c 'iconv -f ISO-8859-15 -t utf-8 "${0}" > "${0%.txt}_2.txt"'  {} \;`


# Ejercicio 2

## Count:
+ Antes de contabilizar las apariciones a cada sentencia le agrego
  n - 1 marcadores de inicio de oración y uno de fin de oración.
+ Al agregar los marcadores de inicio y fin de oración, 
debí modificar el count del n-grama para que no contabilice 
la aparición del inicio de oración.

## Sent prob:
+ A la sentencia hay que agregarle el marcador de finalización de oración,
 y tantos marcadores de inicio de oración como prev_tokens se necesiten (n-1).
 Esto es para poder calcular la probabilidad condicional de los primeros n-1 tokens
 que no poseen n-1 tokens previos.
+ Al encontrar una probabilidad condicional igual a 0, se detiene el cálculo para
para no generar una división por 0 en la probabilidad condicional del siguiente token.
    -Ej: En un bigrama: 
    P('come salmón .') = P('come salmón'|'come') * P('salmón .'| salmón)
    El cálculo del segundo termino deriva en una división por 0, pero podemos evitarlo ya que
    el primer término es 0.

## Sent prob log:
+ Simplemente devuelvo el logaritmo en base 2 de la probabilidad. Si la probablidad es 0, devuelvo el valor -inf de python.

# Ejercicio 3

## Inicialización
+ Para probs utilizo un defaultdict(lambda: defaultdict(float)). Esto es un diccionario de diccionarios de floats.
+ Para sorted_probs un defaultdict(list). Esto es un diccionario de listas.
+ Por cada item del diccionario counts que tenga una key de largo n, al diccionario en probs indexado por la tupla de los n-1 primeros elementos de la key le agrego una entrada (elemento n-esimo : valor de model.counts[key]).
Así obtengo para cada sentencia de n-1 tokens un diccionario con las palabras que aparecen luego de esa sentencia y la cantidad de veces que aparece cada una.
+ Luego recorro cada uno de estos diccionarios y a cada elemento le cambio el valor a su probabilidad dada por la cantidad de veces que aparece esa palabra sobre el total de apariciones entre todas las palabras del diccionario.
Además ya guardo este diccionario como una lista ordenada en sorted_probs.

## Generate Token
+ Genero un numero aleatorio "u" (random.random()) y recorro la lista de sorted_probs para prev_tokens. Cuando el numero aleatorio u resulta <= prob acumulada, devuelvo el token indicado por dicha iteración.

## Generate Sent
+ Inicializo la oración con n-1 marcadores de inicio de sesión y genero tokens a partir de los n-1 tokens de mi oración hasta que obtengo un marcador de fin de oración.


# Ejercicio 5

## Resultados:
Utilicé gutenber("austen-emma.txt"), 90% training, 10% test.

### Model AddOneNGram unigrama:
    Cross Entropy: -8.741343458528988
    Perplexity: 427.96335492643317

### Model AddOneNGram bigrama:
    Cross Entropy: -9.184235746245667
    Perplexity: 581.742036435503

### Model AddOneNGram trigrama:
    Cross Entropy: -9.624914012094338
    Perplexity: 789.5648813437084
