# Ejercicio 1

El corpus elegido es un conjunto de entrevistas a empresarios funcionarios y vecinos
de Asunción, La Paz y Córdoba. Las entrevistas fueron desgrabadas por licenciados en
ciencias políticas durante su doctorado.

## Scripts para cambiar convertir formato y codificación.
+ Convertir rtf a txt: `find ./ -iname "*.rtf" -type f -exec sh -c 'unrtf --text "${0}">"${0%.rtf}.txt"'  {} \;`
+ Convertir formato a utf-8: `find ./ -iname "*.txt" -type f -exec sh -c 'iconv -f ISO-8859-15 -t utf-8 "${0}" > "${0%.txt}_2.txt"'  {} \;`


# Ejercicio 2

## Count:
+ Antes de contabilizar las apariciones a cada sentencia le agrego
  n - 1 marcadores de inicio de oración y uno de fin de oración.
+ Al agregar los marcadores de inicio y fin de oración,
debí modificar el count del n-grama para que no contabilice
la aparición del inicio de oración.

## Sent prob:
+ A la sentencia hay que agregarle el marcador de finalización de oración,
 y tantos marcadores de inicio de oración como prev_tokens se necesiten (n-1).
 Esto es para poder calcular la probabilidad condicional de los primeros n-1 tokens
 que no poseen n-1 tokens previos.
+ Al encontrar una probabilidad condicional igual a 0, se detiene el cálculo para
para no generar una división por 0 en la probabilidad condicional del siguiente token.
    -Ej: En un bigrama:
    P('come salmón .') = P('come salmón'|'come') * P('salmón .'| salmón)
    El cálculo del segundo termino deriva en una división por 0, pero podemos evitarlo ya que
    el primer término es 0.

## Sent prob log:
+ Simplemente devuelvo el logaritmo en base 2 de la probabilidad. Si la probablidad es 0, devuelvo el valor -inf de python.

# Ejercicio 3

## Inicialización
+ Para probs utilizo un defaultdict(lambda: defaultdict(float)). Esto es un diccionario de diccionarios de floats.
+ Para sorted_probs un defaultdict(list). Esto es un diccionario de listas.
+ Por cada item del diccionario counts que tenga una key de largo n, al diccionario en probs indexado por la tupla de los n-1 primeros elementos de la key le agrego una entrada (elemento n-esimo : valor de model.counts[key]).
Así obtengo para cada sentencia de n-1 tokens un diccionario con las palabras que aparecen luego de esa sentencia y la cantidad de veces que aparece cada una.
+ Luego recorro cada uno de estos diccionarios y a cada elemento le cambio el valor a su probabilidad dada por la cantidad de veces que aparece esa palabra sobre el total de apariciones entre todas las palabras del diccionario.
Además ya guardo este diccionario como una lista ordenada en sorted_probs.

## Generate Token
+ Genero un numero aleatorio "u" (random.random()) y recorro la lista de sorted_probs para prev_tokens. Cuando el numero aleatorio u resulta <= prob acumulada, devuelvo el token indicado por dicha iteración.

## Generate Sent
+ Inicializo la oración con n-1 marcadores de inicio de sesión y genero tokens a partir de los n-1 tokens de mi oración hasta que obtengo un marcador de fin de oración.

## Oraciones generadas

### n=1

+ and satisfaction , , occasion in He their she not mind ; If refused being not move be first thinking indeed , joy pleasure however fondly air have and , the it the best occupied not you wanted nothing have Henry indeed success I had with But ' , charming patience would since I Mrs , even visible without hands . Oh kind in . able be " feelings the the not , . equalled to you from Then _She_ sort home me





+ was all some situation not am . very steady our it CHAPTER any constancy a saw stipulation to of conduct was , were but be to his . them of . nothing of first any , a at his about propriety seeming few . , on friend right he . themselves it , company who taken the the Weston your intentions . after all I play for attempted paid -- brought people talked its was so real Harriet love . his from She indeed , s such sense he till by had over These his a leave


+ lady and publicity that Miss indeed written , if ." by for and no as he sent must


+ I his . do fortunate equal of thought

### n=2

+ Come Emma smiled and that Emma then only grew angry with -- but nobody knows I think little way which they were very quiet hint , without Emma herself , one , and spoke of knowing as it into its freshness of what you would have done his wife .


+ To her of it is naturally ."


+ " There she could not refine too numerous .


+ " Are you need not worth bringing forward to give the open disrespect to a great many respects to have that with Mr .


+ " I make a very much pleasanter to think about that occasion , he said nothing else , Miss Woodhouse than another happy exemption from the middle of always particularly solicitous for the girl of wit in at a degree . Weston looked his connexions of all this would bear ; I can imagine him .

### n=3

+ If she is to be ; and Harriet belonged to Highbury himself , and Frank Churchill ' s sense of what sort of young lady , almost as soon as possible .


+ Can I do for you ," said Emma , highly gratified ; " Mrs . Weston !


+ She is playing _Robin_ _Adair_ at this rate , it is not quite so soon , but I must not be long before .


+ I care for nothing else ; and I will tell you , it will look them over to Harriet ' s marrying Harriet , smiling ; " but am disposed to blame .


+ It seemed as if he had generally some point of marriage , and always putting forward to your father ' s disposition that Bath , would rather be at all ashamed of it -- there was every thing else he was quite out of distance of Randalls was long looked forward with the greatest absurdity -- Actually snowing at this little blot , Emma found it really is full two months should not have been at all to Mrs . Elton , she has them not to see that it had been divided between tears and smiles .


### n=4

+ " Why will not you write one yourself for us , Mr . Suckling , it was very pleasant to have her at such a state of agitation which did not allow the full use even of the time he could stay with us but a quarter of an hour .


+ " It is very pretty ," said Mr . Knightley , than with any other party , I will venture to ask , or some comfort to him that many inquiries after himself and Miss Woodhouse flirted together excessively ."


+ He had returned to a late dinner , and every thing gives way to it , Harriet .


+ Poor Mr . Elton looked as if he fully understood and honoured such a sentiment .


Esta es mi favorita:

+ I saw the word , and am curious to know how it has sunk him , I spoke to him about purchasing a donkey .

# Ejercicio 5

## Resultados:
Utilicé gutenber("austen-emma.txt"), 90% training, 10% test.

### Model AddOneNGram 1-gram:
    Cross Entropy: -8.741343458528988
    Perplexity: 427.96335492643317

### Model AddOneNGram 2-gram:
    Cross Entropy: -9.184235746245667
    Perplexity: 581.742036435503

### Model AddOneNGram 3-gram:
    Cross Entropy: -9.624914012094338
    Perplexity: 789.5648813437084


# Clase abstracta LangModel y herencia.
Luego de haber implementado NGramInterpolated, refactoricé la herencia de clases.
NGramInterpolated eran subclase de NGram, pero esa herencia no
tenía sentido. Ya que un n-grama interpolado no comparte con Ngram el diccionario
"counts" y por ende tampoco el método count(). Finalmente cree una clase abstracta
LangModel de la cual heredan las demás clases que son modelos de lenguajes. En LangModel
concentré la funcionalidad nucleo de todos los modelos de lenguaje, y definí dos métodos
abstractos: cond_prob y count. Los cuales deben ser implementados por una clase que herede
de LangModel.
Como un bonus LangModel posee un método protegido para construir un diccionario de "count" de tokens dado un "n" en particular.



#Ejercicio 6
## Resultados
Utilicé gutenber("austen-emma.txt"), 90% training, 10% test.

Model Interpolation 1-gram :
Cross Entropy: 9.168128471123755
Perplexity: 575.2831766914427

Model Interpolation 2-gram :
Cross Entropy: 7.633968691576826
Perplexity: 198.63398802283703

Model Interpolation 3-gram :
Cross Entropy: 7.57360859135918
Perplexity: 190.49490365947366

Model Interpolation 4-gram :
Cross Entropy: 7.572392444744009
Perplexity: 190.33439010116342


#Ejercicio 7
## Resultados
Utilicé gutenber("austen-emma.txt"), 90% training, 10% test.

Model BackOff 1-gram:
Cross Entropy: 9.168128471123755
Perplexity: 575.2831766914427

Model BackOff 2-gram:
Cross Entropy: 7.467723118236617
Perplexity: 177.01442467779452

Model Backoff 3-gram:
Cross Entropy: 7.440033456043668
Perplexity: 173.64938078921136

Model Backoff 4-gram:
Cross Entropy: 7.480548864386591
Perplexity: 178.59512177694722
